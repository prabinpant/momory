---
applyTo: '**'
---

# MOMO: Virtual-Unlimited-Memory AI Agent

## ğŸ¯ Core Mission

Build a production-grade AI agent with **persistent, scalable memory** that operates beyond traditional context window limitations.

### Fundamental Architecture Principles

```
memory â‰  context window
memory = external store + retrieval + compaction + intelligence
```

**Core Thesis**: The LLM is stateless. All memory is external, selective, and retrieved on-demand.

**Current Status**: Phase 2 in progress - Implementing chunking strategy (Clawdbot-inspired)

---

## 0. Key Implementation Decisions

### Storage Architecture

- **Database**: SQLite (single source of truth)
- **Individual Memories**: NOT chunked (already atomic facts)
- **Summaries**: CHUNKED before embedding (400 tokens, 80 overlap)

### Code Style

- **Paradigm**: Functional (no classes)
- **Error Handling**: Result types (no exceptions in core logic)
- **Typing**: Strict TypeScript, explicit return types

### Cost Considerations

- **Optimization**: Embedding cache (LRU, 1000 entries)
- **Future**: Consider batching extraction, skip trivial inputs

---

## 1. System Architecture

### 1.1 Memory Types

#### A. Short-Term Memory (STM)

- **Purpose**: Maintain conversation coherence
- **Storage**: In-memory only (RAM)
- **Lifecycle**: Session-scoped, reset on restart
- **Content**: Recent turns (last 5-10 exchanges)
- **Max Size**: 4KB per entry, ~50KB total

#### B. Long-Term Memory (LTM)

- **Purpose**: Persistent knowledge across sessions
- **Storage**: SQLite + vector embeddings
- **Lifecycle**: Permanent until explicitly pruned
- **Content**: Atomic facts, decisions, preferences, observations
- **Schema**:

```typescript
interface Memory {
  id: string; // UUID
  type: 'fact' | 'decision' | 'preference' | 'observation';
  content: string; // Natural language
  embedding: number[]; // 768-dim vector (Gemini)
  metadata: {
    createdAt: number;
    lastAccessed: number;
    accessCount: number;
    relevanceScore: number; // 0-1, decays over time
    tags: string[];
    source: string; // conversation_id or import
  };
}
```

#### C. Summary Memory (Chunked)

- **Purpose**: Compressed historical context
- **Storage**: SQLite (chunked into focused pieces)
- **Generation**: Triggered when LTM > threshold
- **Content**: Time-windowed summaries, **chunked before embedding**
- **Chunking Strategy**:
  - **Chunk size**: ~400 tokens (~1600 characters)
  - **Overlap**: 80 tokens (~320 characters, 20%)
  - **Why?** Prevents embedding dilution, focused semantics per chunk
- **Compression Ratio**: Target 10:1 (raw memories â†’ summary chunks)

**Chunking Process** (inspired by Clawdbot):

1. Generate summary text from old memories
2. Split into overlapping chunks (~400 tokens each)
3. Embed each chunk separately (focused embeddings!)
4. Store chunks with line ranges for retrieval
5. Vector search retrieves relevant chunks only

---

## 1.2 Chunking Strategy (Critical for Accuracy)

### Why Chunk?

**Problem**: Long summaries with mixed topics create diluted embeddings

```typescript
// âŒ BAD: Single embedding for mixed content
const summary = 'User is a Python developer. Loves pizza. Traveled to Japan.';
const embedding = await embed(summary);
// â†’ [0.345, -0.234, ...] (33% programming, 33% food, 33% travel - DILUTED!)

Query: 'What programming languages?';
similarity(query, embedding) = 0.67; // Low due to dilution
```

**Solution**: Chunk before embedding

```typescript
// âœ… GOOD: Focused embeddings per chunk
const chunk1 = "User is a Python developer with 5 years experience...";
const chunk2 = "User loves pizza and Italian cuisine...";
const chunk3 = "User traveled to Japan and loved Kyoto...";

const emb1 = await embed(chunk1); // 100% programming semantics
const emb2 = await embed(chunk2); // 100% food semantics
const emb3 = await embed(chunk3); // 100% travel semantics

Query: "What programming languages?"
similarity(query, emb1) = 0.94 âœ… High!
similarity(query, emb2) = 0.21 (filtered out)
```

### Chunking Implementation

```typescript
interface ChunkOptions {
  maxChars: number; // ~400 tokens = 1600 chars
  overlapChars: number; // ~80 tokens = 320 chars (20% overlap)
}

// Overlap ensures context continuity:
// Chunk 1: "...Decision: use REST for simplicity."
// Chunk 2: "Decision: use REST for simplicity. Deployed to prod..."
//           â†‘ Shared content preserves context
```

---

## 2. Agent Execution Loop

### Complete Pipeline (Non-Negotiable Sequence)

```typescript
async function processUserInput(input: string): Promise<string> {
  // STEP 1: Receive Input
  const userMessage = input;

  // STEP 2: Retrieve Relevant Memory
  const inputEmbedding = await embedText(input);
  const relevantMemories = await vectorSearch(inputEmbedding, (topK = 10));
  const summaryContext = await getLatestSummary();
  const recentContext = getSTM();

  // STEP 3: Construct Context-Rich Prompt
  const prompt = buildPrompt({
    system: AGENT_SYSTEM_PROMPT,
    relevantMemories,
    summaryContext,
    recentContext,
    userMessage,
  });

  // STEP 4: LLM Reasoning (Gemini)
  const response = await gemini.generate(prompt);

  // STEP 5: Extract Memory Candidates
  const memoryExtractionPrompt = buildMemoryExtractionPrompt(
    userMessage,
    response
  );
  const memories = await gemini.generate(memoryExtractionPrompt);

  // STEP 6: Persist Valuable Memories
  const validMemories = filterNoise(memories);
  await saveToLTM(validMemories);
  await updateVectorIndex(validMemories);

  // STEP 7: Maintenance (Conditional)
  if (shouldRunMaintenance()) {
    await summarizeOldMemories();
    await decayRelevanceScores();
    await pruneStaleMemories();
  }

  // STEP 8: Update STM
  addToSTM({ role: 'user', content: userMessage });
  addToSTM({ role: 'assistant', content: response });

  return response;
}
```

---

## 3. Prompt Engineering (Critical)

### 3.1 System Prompt Template

```
You are MOMO, an AI agent with persistent external memory.

CRITICAL CAPABILITIES:
- You remember facts, preferences, and decisions across sessions
- You retrieve only relevant memories, not entire history
- You distinguish between important and trivial information
- You reason with both retrieved context and current input

MEMORY CONTEXT (Retrieved for this task):
{relevantMemories}

HISTORICAL SUMMARY:
{summaryContext}

RECENT CONVERSATION:
{recentContext}

CURRENT TASK:
{userMessage}

RESPONSE GUIDELINES:
1. Use retrieved memories naturally in your response
2. Acknowledge when you're recalling past information
3. If memories conflict, prefer recent over old
4. If no relevant memory exists, admit it
5. Propose new facts worth remembering
```

### 3.2 Memory Extraction Prompt

```
Analyze the following interaction and extract structured memories.

USER INPUT: {userMessage}
ASSISTANT RESPONSE: {assistantResponse}

EXTRACTION RULES:
1. Only extract objectively valuable information
2. Avoid extracting: greetings, confirmations, trivial acknowledgments
3. Prefer atomic facts over long narratives
4. Tag each memory with type: fact, decision, preference, observation

OUTPUT FORMAT (JSON):
{
  "memories": [
    {
      "type": "fact|decision|preference|observation",
      "content": "Single, clear statement",
      "confidence": 0.0-1.0,
      "tags": ["tag1", "tag2"]
    }
  ],
  "shouldSummarize": boolean,
  "reasoning": "Brief explanation of extraction decisions"
}

EXAMPLES:
âœ… GOOD: "User prefers Python for data tasks"
âŒ BAD: "User said they like Python"
âœ… GOOD: "Project deadline is March 15, 2026"
âŒ BAD: "We talked about deadlines"
```

### 3.3 Summarization Prompt (with Chunking)

```
Summarize the following memories into a concise overview.

MEMORIES TO SUMMARIZE (30 old memories):
1. User prefers Python for data science
2. User's project deadline is March 15, 2026
3. User chose PostgreSQL over MySQL
... (27 more)

SUMMARIZATION RULES:
1. Organize by topic/theme (programming, preferences, work, etc.)
2. Preserve all critical facts, dates, and decisions
3. Remove redundant information
4. Use clear, atomic statements
5. Target ~500-800 words

OUTPUT:
A well-organized summary that will be chunked for embedding.
Ensure natural topic transitions for effective chunking.
```

**Post-Processing:**

```typescript
const summary = await generateText(summarizationPrompt);

// Chunk the summary (400 tokens with 80 token overlap)
const chunks = chunkText(summary, {
  maxChars: 1600,
  overlapChars: 320,
});

// Embed each chunk separately
for (const chunk of chunks) {
  const embedding = await generateEmbedding(chunk.text);
  await saveSummaryChunk({
    content: chunk.text,
    embedding,
    startLine: chunk.startLine,
    endLine: chunk.endLine,
  });
}
```

---

## 4. Code Quality Standards

### 4.1 TypeScript Conventions

- **Strict Mode**: Always enable `strict: true`
- **No Any**: Avoid `any`, use `unknown` or proper types
- **Explicit Returns**: Always declare return types
- **Null Safety**: Use optional chaining and nullish coalescing
- **Immutability**: Prefer `const`, use `readonly` for interfaces

### 4.2 Error Handling

```typescript
// ALWAYS use Result types, never throw in core logic
type Result<T, E = Error> = { ok: true; value: T } | { ok: false; error: E };

// Example
async function embedText(text: string): Promise<Result<number[]>> {
  try {
    const embedding = await gemini.embed(text);
    return { ok: true, value: embedding };
  } catch (error) {
    return { ok: false, error: new Error('Embedding failed') };
  }
}
```

### 4.3 Logging Strategy

```typescript
// Use structured logging
logger.info('Memory retrieved', {
  queryEmbeddingTime: 45,
  resultsCount: 10,
  topRelevance: 0.89,
  cacheHit: false,
});

// Never log sensitive data
logger.debug('User input received', { length: input.length }); // âœ…
logger.debug('User input', { input }); // âŒ
```

### 4.4 File Structure

```
momo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ agent.ts           # Main execution loop
â”‚   â”‚   â”œâ”€â”€ memory.ts          # Memory manager
â”‚   â”‚   â””â”€â”€ prompts.ts         # Prompt templates
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ gemini.ts          # Gemini API wrapper
â”‚   â”‚   â”œâ”€â”€ embeddings.ts      # Vector operations
â”‚   â”‚   â””â”€â”€ storage.ts         # SQLite layer
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”œâ”€â”€ memory.ts          # Memory interfaces
â”‚   â”‚   â””â”€â”€ index.ts           # Shared types
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.ts          # Logging
â”‚   â”‚   â””â”€â”€ config.ts          # Configuration
â”‚   â””â”€â”€ index.ts               # CLI entry point
â”œâ”€â”€ db/
â”‚   â””â”€â”€ schema.sql             # Database schema
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ *.test.ts
â”œâ”€â”€ .env.example
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

---

## 5. Performance Targets

| Metric               | Target              | Critical Threshold |
| -------------------- | ------------------- | ------------------ |
| Query latency        | < 200ms             | < 500ms            |
| Embedding time       | < 100ms             | < 300ms            |
| Memory retrieval     | < 50ms              | < 150ms            |
| Context construction | < 10ms              | < 50ms             |
| Prompt size          | < 4KB               | < 8KB              |
| LTM database size    | < 100MB/1K memories | < 1GB              |
| Memory per session   | < 512MB             | < 1GB              |

---

## 6. Testing Requirements

### 6.1 Core Tests (Non-Negotiable)

1. **Memory Persistence**: Restart â†’ Recall test
2. **Retrieval Accuracy**: Relevant results in top-K
3. **Noise Filtering**: Trivial input â†’ No storage
4. **Summarization**: 100 memories â†’ 10 summary entries
5. **Prompt Bounds**: Never exceed 8KB context
6. **Decay Logic**: Old memories decrease relevance
7. **Conflict Resolution**: Recent overrides old

### 6.2 Integration Tests

```typescript
describe('Agent Memory System', () => {
  it('should remember facts across restarts', async () => {
    await agent.process('My name is Alice');
    await agent.restart();
    const response = await agent.process('What is my name?');
    expect(response).toContain('Alice');
  });

  it('should retrieve only relevant memories', async () => {
    await agent.process('I love pizza');
    await agent.process('I hate rain');
    const memories = await agent.retrieveMemories('food preferences');
    expect(memories).toContainEqual({ content: 'I love pizza' });
    expect(memories).not.toContainEqual({ content: 'I hate rain' });
  });
});
```

---

## 7. Implementation Phases

### Phase 1: Foundation

- [x] Initialize TypeScript project
- [x] Setup Gemini API client
- [x] Create SQLite schema
- [x] Implement basic vector similarity

### Phase 2: Core Loop

- [x] Build memory storage layer
- [ ] Implement chunking strategy (Clawdbot-inspired)
- [ ] Build memory retrieval with chunked search
- [ ] Create prompt construction
- [ ] Connect Gemini reasoning
- [ ] Build agent execution pipeline

### Phase 3: Intelligence

- [ ] Memory extraction logic

### Phase 3: Intelligence

- [ ] Memory extraction logic
- [ ] Noise filtering rules
- [ ] Relevance scoring
- [ ] Summarization engine with chunking

### Phase 4: Scale & Polish

- [ ] Memory maintenance (decay, pruning)
- [ ] CLI interface
- [ ] Logging & debugging
- [ ] Performance optimization

---

## 8. Success Criteria (Exit Conditions)

**You are done when ALL are true:**

1. âœ… Agent remembers facts across process restarts
2. âœ… Retrieval returns relevant context (not random)
3. âœ… Chunked summaries prevent embedding dilution
4. âœ… Prompt size stays < 8KB regardless of memory size
5. âœ… Old memories are compressed into chunked summaries
6. âœ… Memory improves answer quality (measurable)
7. âœ… No crashes with 1000+ memories
8. âœ… Maintenance runs without blocking UX

---

## 9. Anti-Patterns (Never Do This)

âŒ **Don't dump entire conversation history into prompts**
âœ… Retrieve top-K relevant memories only

âŒ **Don't store everything the user says**
âœ… Filter for meaningful, reusable information

âŒ **Don't use LLM context as memory**
âœ… Context is temporary, memory is persistent

âŒ **Don't block on maintenance operations**
âœ… Run cleanup in background or async

âŒ **Don't ignore error handling**
âœ… Use Result types, never throw in core paths

âŒ **Don't hardcode prompts in logic files**
âœ… Centralize in `prompts.ts` as templates

---

## 10. Configuration

### Environment Variables

```bash
# Gemini API
GEMINI_API_KEY=your_key_here
GEMINI_MODEL=gemini-2.0-flash-exp  # or gemini-pro

# Memory Settings
MEMORY_DB_PATH=./db/memory.db
MEMORY_TOP_K=10
MEMORY_RELEVANCE_THRESHOLD=0.65
MEMORY_DECAY_RATE=0.05  # per week
MEMORY_SUMMARY_THRESHOLD=100  # memories

# Performance
MAX_PROMPT_SIZE=8192  # bytes
EMBEDDING_CACHE_SIZE=1000
VECTOR_SEARCH_BATCH_SIZE=50

# Logging
LOG_LEVEL=info  # debug, info, warn, error
LOG_FILE=./logs/momo.log
```

---

## 11. Development Workflow

1. **Start with tests**: Write test first, then implement
2. **Type everything**: No `any`, explicit return types
3. **Log extensively**: Structured logs for debugging
4. **Profile regularly**: Check memory/CPU usage
5. **Review prompts**: Test with real examples
6. **Validate retrieval**: Check top-K relevance
7. **Monitor maintenance**: Ensure cleanup runs

---

## 12. References

- **[Clawdbot](https://github.com/clawdbot/clawdbot)**: Chunking strategy, memory architecture inspiration
- **[Manthan Gupta's Article](https://x.com/manthanguptaa/status/2015780646770323543)**: How Clawdbot Remembers Everything
- **Vector Similarity**: Cosine similarity for embeddings
- **SQLite**: Persistent storage with vector operations
- **Gemini API**: Text generation + embeddings
- **Result Types**: Railway-oriented programming

---

**Last Updated**: February 6, 2026
**Version**: 1.1.0
**Status**: Implementation In Progress (Chunking Strategy Added)

---

**Last Updated**: February 6, 2026
**Version**: 1.0.0
**Status**: Implementation Ready
